---
title: ATRank - An Attention-Based User Behavior Modeling Framework for Recommendation
authors:
- Chang Zhou
- Jinze Bai
- Junshuai Song
- Xiaofei Liu
- Zhengchao Zhao
- Xiusi Chen
- Jun Gao
fieldsOfStudy:
- Computer Science
meta_key: 2018-atrank-an-attention-based-user-behavior-modeling-framework-for-recommendation
numCitedBy: 206
reading_status: TBD
ref_count: 33
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/ATRank:-An-Attention-Based-User-Behavior-Modeling-Zhou-Bai/063598cdaff79852c3647d074506120889c5c17b?sort=total-citations
venue: AAAI
year: 2018
---

[semanticscholar url](https://www.semanticscholar.org/paper/ATRank:-An-Attention-Based-User-Behavior-Modeling-Zhou-Bai/063598cdaff79852c3647d074506120889c5c17b?sort=total-citations)

# ATRank - An Attention-Based User Behavior Modeling Framework for Recommendation

## Abstract

A user can be represented as what he/she does along the history. A common way to deal with the user modeling problem is to manually extract all kinds of aggregated features over the heterogeneous behaviors, which may fail to fully represent the data itself due to limited human instinct. Recent works usually use RNN-based methods to give an overall embedding of a behavior sequence, which then could be exploited by the downstream applications. However, this can only preserve very limited information, or aggregated memories of a person. When a downstream application requires to facilitate the modeled user features, it may lose the integrity of the specific highly correlated behavior of the user, and introduce noises derived from unrelated behaviors. This paper proposes an attention based user behavior modeling framework called ATRank, which we mainly use for recommendation tasks. Heterogeneous user behaviors are considered in our model that we project all types of behaviors into multiple latent semantic spaces, where influence can be made among the behaviors via self-attention. Downstream applications then can use the user behavior vectors via vanilla attention. Experiments show that ATRank can achieve better performance and faster training process. We further explore ATRank to use one unified model to predict different types of user behaviors at the same time, showing a comparable performance with the highly optimized individual models.

## Paper References

1. Joint Deep Modeling of Users and Items Using Reviews for Recommendation
2. Deep Interest Network for Click-Through Rate Prediction
3. Image-Based Recommendations on Styles and Substitutes
4. DeepIntent - Learning Attentions for Online Advertising with Recurrent Neural Networks
5. [Wide & Deep Learning for Recommender Systems](2016-wide-deep-learning-for-recommender-systems.md)
6. Context-aware music recommendation based on latenttopic sequential patterns
7. Fast context-aware recommendations with factorization machines
8. [BPR - Bayesian Personalized Ranking from Implicit Feedback](2009-bpr-bayesian-personalized-ranking-from-implicit-feedback.md)
9. [Attention is All you Need](2017-attention-is-all-you-need.md)
10. Heterogeneous Network Embedding via Deep Architectures
11. Attention-over-Attention Neural Networks for Reading Comprehension
12. Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks
13. Knowledge Graph Embedding by Translating on Hyperplanes
14. [A Structured Self-attentive Sentence Embedding](2017-a-structured-self-attentive-sentence-embedding.md)
15. [Deep Neural Networks for YouTube Recommendations](2016-deep-neural-networks-for-youtube-recommendations.md)
16. [Effective Approaches to Attention-based Neural Machine Translation](2015-effective-approaches-to-attention-based-neural-machine-translation.md)
17. Learning Entity and Relation Embeddings for Knowledge Graph Completion
18. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality.md)
19. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention.md)
20. [Long Short-Term Memory](1997-long-short-term-memory.md)
21. [Long Short-Term Memory-Networks for Machine Reading](2016-long-short-term-memory-networks-for-machine-reading.md)
22. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](2014-empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling.md)
23. [Convolutional Sequence to Sequence Learning](2017-convolutional-sequence-to-sequence-learning.md)
24. [Multimodal Deep Learning](2011-multimodal-deep-learning.md)
25. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
26. Association for the Advancement of Artificial Intelligence
27. Personal recommendation using deep recurrent neural networks in NetEase
28. [Show and tell - A neural image caption generator](2015-show-and-tell-a-neural-image-caption-generator.md)
29. Translating Embeddings for Modeling Multi-relational Data
30. [Convolutional Neural Networks for Sentence Classification](2014-convolutional-neural-networks-for-sentence-classification.md)
