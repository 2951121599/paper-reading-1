---
title: Long Short-Term Memory-Networks for Machine Reading
authors:
- Jianpeng Cheng
- Li Dong
- Mirella Lapata
fieldsOfStudy:
- Computer Science
meta_key: 2016-long-short-term-memory-networks-for-machine-reading
numCitedBy: 771
reading_status: TBD
ref_count: 74
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Long-Short-Term-Memory-Networks-for-Machine-Reading-Cheng-Dong/13fe71da009484f240c46f14d9330e932f8de210?sort=total-citations
venue: EMNLP
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/Long-Short-Term-Memory-Networks-for-Machine-Reading-Cheng-Dong/13fe71da009484f240c46f14d9330e932f8de210?sort=total-citations)

# Long Short-Term Memory-Networks for Machine Reading

## Abstract

In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.

## Paper References

1. [Memory Networks](2015-memory-networks.md)
2. [Ask Me Anything - Dynamic Memory Networks for Natural Language Processing](2016-ask-me-anything-dynamic-memory-networks-for-natural-language-processing.md)
3. Recurrent Memory Networks for Language Modeling
4. Recurrent Memory Network for Language Modeling
5. [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](2015-improved-semantic-representations-from-tree-structured-long-short-term-memory-networks.md)
6. [A Convolutional Neural Network for Modelling Sentences](2014-a-convolutional-neural-network-for-modelling-sentences.md)
7. [Dynamic Memory Networks for Visual and Textual Question Answering](2016-dynamic-memory-networks-for-visual-and-textual-question-answering.md)
8. Depth-Gated LSTM
9. A Deep Memory-based Architecture for Sequence-to-Sequence Learning
10. [Grid Long Short-Term Memory](2016-grid-long-short-term-memory.md)
11. Depth-Gated Recurrent Neural Networks
12. Learning Natural Language Inference with LSTM
13. [Teaching Machines to Read and Comprehend](2015-teaching-machines-to-read-and-comprehend.md)
14. Learning to Execute
15. Learning Longer Memory in Recurrent Neural Networks
16. A Clockwork RNN
17. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
18. [Long Short-Term Memory](1997-long-short-term-memory.md)
19. [Learning to Compose Neural Networks for Question Answering](2016-learning-to-compose-neural-networks-for-question-answering.md)
20. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
21. Reasoning about Entailment with Neural Attention
22. [A large annotated corpus for learning natural language inference](2015-a-large-annotated-corpus-for-learning-natural-language-inference.md)
23. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection
24. Learning to Transduce with Unbounded Memory
25. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
26. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model.md)
27. [A Neural Attention Model for Abstractive Sentence Summarization](2015-a-neural-attention-model-for-abstractive-sentence-summarization.md)
28. Learning long-term dependencies with gradient descent is difficult
29. Molding CNNs for text - non-linear, non-consecutive convolutions
30. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation.md)
31. Gated Feedback Recurrent Neural Networks
32. Toward an Architecture for Never-Ending Language Learning
33. Machine Reading - A Killer App for Statistical Relational AI
34. [Distributed Representations of Sentences and Documents](2014-distributed-representations-of-sentences-and-documents.md)
35. Insensitivity of the Human Sentence-Processing System to Hierarchical Structure
36. Transition-Based Dependency Parsing with Stack Long Short-Term Memory
37. Machine Reading
38. DeepDive - Web-scale Knowledge-base Construction using Statistical Learning and Inference
39. Locality and Parsing Complexity
40. Open Information Extraction - The Second Generation
41. Recovery from misanalyses of garden-path sentences â˜†
42. Learning Context-free Grammars
43. Unsupervised Ontology Induction from Text
44. A computational model of human parsing
45. [Identifying Relations for Open Information Extraction](2011-identifying-relations-for-open-information-extraction.md)
46. Corpus-Based Induction of Syntactic Structure - Models of Dependency and Constituency
47. [On the difficulty of training recurrent neural networks](2013-on-the-difficulty-of-training-recurrent-neural-networks.md)
48. Injecting Logical Background Knowledge into Embeddings for Relation Extraction
49. Integration of visual and linguistic information in spoken language comprehension.
50. On the Computational Power of Neural Nets
51. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization.md)
52. Filling Knowledge Gaps in Text for Machine Reading
53. Knowledge Base Population - Successful Approaches and Challenges
54. Extracting and evaluating general world knowledge from the Brown Corpus
55. A study of the knowledge base requirements for passing an elementary science test
56. The PASCAL Recognising Textual Entailment Challenge
57. Eye movements in reading and information processing - 20 years of research.
58. [End-To-End Memory Networks](2015-end-to-end-memory-networks.md)
59. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks.md)
60. Untersuchungen zu dynamischen neuronalen Netzen
61. A Fast Unified Model for Parsing and Sentence Understanding
62. Deep Recursive Neural Networks for Compositionality in Language
63. Learning Context-free Grammars - Capabilities and Limitations of a Recurrent Neural Network with an External Stack Memory
64. Learning Structured Embeddings of Knowledge Bases
65. [Convolutional Neural Networks for Sentence Classification](2014-convolutional-neural-networks-for-sentence-classification.md)
